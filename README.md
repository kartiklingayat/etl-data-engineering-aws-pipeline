# ğŸš€ ETL Data Engineering Pipeline on AWS

This project demonstrates a complete **end-to-end ETL Data Engineering Pipeline**
built using **AWS Glue, PySpark, Amazon S3, Glue Data Catalog, and Amazon Redshift**.

The goal of this project is to show how raw data is:
**Extracted â†’ Transformed â†’ Loaded â†’ Queried â†’ Analyzed**  
using a fully serverless and scalable cloud architecture.

---

# ğŸ—ï¸ Architecture Overview
(Replace this with your actual screenshot)

![architecture](screenshots/architecture.png)

---

# ğŸ“ Project Structure


etl-data-engineering-aws-pipeline/
â”‚â”€â”€ data/
â”‚ â””â”€â”€ marketing_campaign.csv
â”‚
â”‚â”€â”€ notebooks/
â”‚ â””â”€â”€ etl-project-transform-data.ipynb
â”‚
â”‚â”€â”€ src/
â”‚ â””â”€â”€ pyspark_etl_script.py
â”‚
â”‚â”€â”€ screenshots/
â”‚ â”œâ”€â”€ architecture.png
â”‚ â”œâ”€â”€ extract.png
â”‚ â”œâ”€â”€ transform.png
â”‚ â”œâ”€â”€ load.png
â”‚
â”‚â”€â”€ README.md
â”‚â”€â”€ .gitignore
â”‚â”€â”€ .gitattributes


---

# ğŸ”„ ETL Pipeline Stages  
**Extract â†’ Transform â†’ Load**

---

# ğŸŸ§ PART 1 â€” EXTRACT  
_Load Raw Data into AWS S3_

![extract](screenshots/extract.png)

---

## âœ” 1. Create IAM Role for AWS Glue

AWS Console â†’ IAM â†’ Roles â†’ Create Role  
Service: **Glue**  
Permissions: **AdministratorAccess**

**Role Name:**

IAM-Role-etl-project


---

## âœ” 2. Create S3 Bucket & Folders

Bucket Name:

etl-project-for-medium


Inside folder structure:

etl-project-for-medium-database/
â”œâ”€â”€ raw_data/
â””â”€â”€ transformed_data/


Upload dataset:

marketing_campaign.csv


---

## âœ” 3. Create Glue Database & Table (using Crawler)

### 3.1 Create Database
AWS Glue â†’ Data Catalog â†’ Databases â†’ Add Database  

etl-project-for-medium-database


### 3.2 Create Glue Crawler
Crawler Name:

etl-project-for-medium-crawler

Source: raw_data folder  
IAM Role: `IAM-Role-etl-project`  
Target DB: `etl-project-for-medium-database`

Run crawler â†’ table created.

---

# ğŸŸ¦ PART 2 â€” TRANSFORM  
_Transform data using PySpark on AWS Glue_

![transform](screenshots/transform.png)

---

## âœ” 4. Create AWS Glue Interactive Notebook

Job Name:

etl-project-for-medium-job


IAM Role: `IAM-Role-etl-project`  
Kernel: Spark  
Workers: 5  
Worker Type: G.1X

---

# ğŸ§ª 5. PySpark Code (No Changes Done)

## â–¶ 5.1 Initialize Session
```python
%idle_timeout 2880
%glue_version 3.0
%worker_type G.1X
%number_of_workers 5

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

sc = SparkContext.getOrCreate()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
â–¶ 5.2 Load Data From Glue Catalog
dyf = glueContext.create_dynamic_frame.from_catalog(
    database='etl-project-for-medium-database',
    table_name='raw_data'
)
dyf.printSchema()
â–¶ 5.3 Convert to DataFrame
df = dyf.toDF()
df.show()
â–¶ 5.4 Select Required Columns
df = df["id","year_birth","education","marital_status","income","dt_customer"]
df.show()
â–¶ 5.5 Check NULL Values
from pyspark.sql.functions import *
df.select([count(when(col(c).isNull(),c)).alias(c) for c in df.columns]).show()
â–¶ 5.6 Fill NULL Income with Mean
mean_value = df.select(mean(col('income'))).collect()[0][0]
df = df.fillna(mean_value, subset=['income'])
df.select([count(when(col(c).isNull(),c)).alias(c) for c in df.columns]).show()
â–¶ 5.7 Save Transformed Data (CSV)
df.write \
  .format("csv") \
  .mode("append") \
  .option("header", "true") \
  .save("s3://etl-project-for-medium/etl-project-for-medium-database/transformed_data/")
â–¶ 5.8 Save Transformed Data (JSON)
df.write \
 .format("json") \
 .mode("append") \
 .save("s3://etl-project-for-medium/etl-project-for-medium-database/transformed_data/")
ğŸŸ© PART 3 â€” LOAD

Load transformed data into Amazon Redshift

âœ” 6. Create IAM Role for Redshift

Service: Redshift
Permission: AdministratorAccess

Role Name:

IAM-Role-etl-project-redshift
âœ” 7. Create Amazon Redshift Cluster

Cluster ID:

etl-project-cluster

Node type: dc2.large
Nodes: 1
Attach IAM Role: IAM-Role-etl-project-redshift

ğŸŸ¥ 7 â€” Load Data into Redshift
â–¶ 7.1 Create Table
CREATE TABLE etl_project_transformed_data_table(
"id" INTEGER NULL,
"year_birth" INTEGER NULL,
"education" VARCHAR NULL,
"marital_status" VARCHAR NULL,
"income" INTEGER NULL,
"dt_customer" DATE NULL
) ENCODE AUTO;
â–¶ 7.2 COPY Data from S3 into Redshift
COPY etl_project_transformed_data_table
FROM 's3://etl-project-for-medium/etl-project-for-medium-database/transformed_data/part-00000-6429f588-c5f4-4f6e-88df-b8bd3506113e-c000.csv'
IAM_ROLE 'arn:aws:iam::835769464848:role/IAM-Role-etl-project-redshift'
IGNOREHEADER 1
DELIMITER ',';
â–¶ 7.3 Verify Table
SELECT * FROM etl_project_transformed_data_table;
â–¶ 7.4 Analytics Query
SELECT education, COUNT(id), AVG(income)
FROM etl_project_transformed_data_table
GROUP BY education;
